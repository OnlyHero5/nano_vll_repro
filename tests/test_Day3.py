"""Day 3 æµ‹è¯•è„šæœ¬ - PagedAttention å’Œ Block Manager"""

import sys
sys.path.insert(0, '.')

import torch
from engine.sequence import Sequence, SequenceStatus
from engine.block_manager import Block, BlockManager
from layers.attention import Attention, store_kvcache
from utils.context import set_context, reset_context
from sampling_params import SamplingParams


@torch.inference_mode()
def test_block():
    """æµ‹è¯• Block ç±»"""
    print("=" * 50)
    print("æµ‹è¯• Block")
    print("=" * 50)
    
    block = Block(block_id=0)
    print(f"åˆå§‹çŠ¶æ€: {block}")
    assert block.ref_count == 0
    assert block.hash == -1
    
    # æ¨¡æ‹Ÿåˆ†é…
    block.reset()
    print(f"åˆ†é…å: {block}")
    assert block.ref_count == 1
    
    # æ¨¡æ‹Ÿæ›´æ–°å“ˆå¸Œ
    token_ids = [100, 200, 300, 400]
    block.update(hash_value=12345, token_ids=token_ids)
    print(f"æ›´æ–°å“ˆå¸Œå: {block}")
    assert block.hash == 12345
    assert block.token_ids == token_ids
    
    print("âœ… Block æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_block_manager_basic():
    """æµ‹è¯• BlockManager åŸºç¡€åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯• BlockManager åŸºç¡€åŠŸèƒ½")
    print("=" * 50)
    
    num_blocks = 10
    block_size = 4  # å°å°ºå¯¸ä¾¿äºæµ‹è¯•
    
    manager = BlockManager(num_blocks=num_blocks, block_size=block_size)
    print(f"åˆå§‹çŠ¶æ€: {manager}")
    assert manager.get_num_free_blocks() == num_blocks
    
    # åˆ›å»ºåºåˆ—
    token_ids = [1, 2, 3, 4, 5, 6, 7]  # 7 tokens, éœ€è¦ 2 blocks
    seq = Sequence(token_ids, SamplingParams())
    seq.block_size = block_size  # è¦†ç›–é»˜è®¤çš„ 256
    
    print(f"åºåˆ—éœ€è¦ {seq.num_blocks} ä¸ª blocks")
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…
    assert manager.can_allocate(seq)
    
    # åˆ†é…
    manager.allocate(seq)
    print(f"åˆ†é…å: {manager}")
    print(f"åºåˆ— block_table: {seq.block_table}")
    
    assert len(seq.block_table) == 2
    assert manager.get_num_free_blocks() == num_blocks - 2
    
    # é‡Šæ”¾
    manager.deallocate(seq)
    print(f"é‡Šæ”¾å: {manager}")
    assert manager.get_num_free_blocks() == num_blocks
    assert len(seq.block_table) == 0
    
    print("âœ… BlockManager åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_block_manager_append():
    """æµ‹è¯• BlockManager append_slot åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯• BlockManager append_slot")
    print("=" * 50)
    
    num_blocks = 10
    block_size = 4
    
    manager = BlockManager(num_blocks=num_blocks, block_size=block_size)
    
    # åˆ›å»ºåˆå§‹åºåˆ—
    token_ids = [1, 2, 3]  # 3 tokens, 1 block
    seq = Sequence(token_ids, SamplingParams())
    seq.block_size = block_size
    
    manager.allocate(seq)
    print(f"åˆå§‹: {len(seq)} tokens, {len(seq.block_table)} blocks")
    
    # æ¨¡æ‹Ÿ decodeï¼šè¿½åŠ  tokens
    for new_token in [4, 5, 6, 7, 8]:
        seq.append_token(new_token)
        manager.append_slot(seq)
        print(f"è¿½åŠ  token {new_token}: {len(seq)} tokens, {len(seq.block_table)} blocks")
    
    assert len(seq.block_table) == 2  # 8 tokens = 2 blocks
    
    print("âœ… BlockManager append_slot æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_slot_mapping():
    """æµ‹è¯• slot mapping è®¡ç®—"""
    print("=" * 50)
    print("æµ‹è¯• Slot Mapping")
    print("=" * 50)
    
    num_blocks = 10
    block_size = 4
    
    manager = BlockManager(num_blocks=num_blocks, block_size=block_size)
    
    # åˆ›å»ºåºåˆ—
    token_ids = [1, 2, 3, 4, 5, 6]
    seq = Sequence(token_ids, SamplingParams())
    seq.block_size = block_size
    
    manager.allocate(seq)
    
    # è®¡ç®— slot mapping
    slots = manager.get_slot_mapping(seq)
    print(f"Token IDs: {token_ids}")
    print(f"Block Table: {seq.block_table}")
    print(f"Slot Mapping: {slots}")
    
    # éªŒè¯ slot è®¡ç®—
    for i, slot in enumerate(slots):
        block_idx = i // block_size
        offset = i % block_size
        expected_slot = seq.block_table[block_idx] * block_size + offset
        assert slot == expected_slot, f"Token {i}: expected {expected_slot}, got {slot}"
    
    print("âœ… Slot Mapping æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_prefix_cache():
    """æµ‹è¯• Prefix Caching"""
    print("=" * 50)
    print("æµ‹è¯• Prefix Caching")
    print("=" * 50)
    
    num_blocks = 20
    block_size = 4
    
    manager = BlockManager(num_blocks=num_blocks, block_size=block_size)
    
    # ç¬¬ä¸€ä¸ªåºåˆ—
    prefix_tokens = [100, 200, 300, 400]  # å®Œæ•´çš„ä¸€ä¸ª block
    seq1_tokens = prefix_tokens + [1, 2]
    seq1 = Sequence(seq1_tokens, SamplingParams())
    seq1.block_size = block_size
    
    manager.allocate(seq1)
    print(f"Seq1 block_table: {seq1.block_table}")
    print(f"Seq1 num_cached_tokens: {seq1.num_cached_tokens}")
    
    # ç¬¬äºŒä¸ªåºåˆ—ï¼ˆå…±äº«å‰ç¼€ï¼‰
    seq2_tokens = prefix_tokens + [3, 4, 5]
    seq2 = Sequence(seq2_tokens, SamplingParams())
    seq2.block_size = block_size
    
    manager.allocate(seq2)
    print(f"Seq2 block_table: {seq2.block_table}")
    print(f"Seq2 num_cached_tokens: {seq2.num_cached_tokens}")
    
    # éªŒè¯ï¼šç¬¬ä¸€ä¸ª block åº”è¯¥è¢«å…±äº«
    assert seq1.block_table[0] == seq2.block_table[0], "First block should be shared!"
    assert seq2.num_cached_tokens == block_size, "Should have cached the prefix"
    
    # éªŒè¯å¼•ç”¨è®¡æ•°
    shared_block_id = seq1.block_table[0]
    assert manager.blocks[shared_block_id].ref_count == 2
    
    print("âœ… Prefix Caching æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_attention_with_context():
    """æµ‹è¯• Attention å±‚ä¸ Context"""
    print("=" * 50)
    print("æµ‹è¯• Attention ä¸ Context")
    print("=" * 50)
    
    device = "cuda"
    dtype = torch.bfloat16

    num_heads = 4
    num_kv_heads = 2
    head_dim = 32
    
    attn = Attention(
        num_heads=num_heads,
        head_dim=head_dim,
        scale=head_dim ** -0.5,
        num_kv_heads=num_kv_heads,
    )
    
    # æµ‹è¯• Prefill
    num_tokens = 5
    q = torch.randn(num_tokens, num_heads, head_dim, device=device,dtype=dtype)
    k = torch.randn(num_tokens, num_kv_heads, head_dim, device=device,dtype=dtype)
    v = torch.randn(num_tokens, num_kv_heads, head_dim, device=device,dtype=dtype)
    
    # è®¾ç½® Contextï¼ˆæ—  KV Cache çš„ç®€å•æƒ…å†µï¼‰
    cu_seqlens = torch.tensor([0, num_tokens], dtype=torch.int32, device=device)
    set_context(
        is_prefill=True,
        cu_seqlens_q=cu_seqlens,
        cu_seqlens_k=cu_seqlens,
        max_seqlen_q=num_tokens,
        max_seqlen_k=num_tokens,
        slot_mapping=torch.arange(num_tokens),
    )
    
    # Forwardï¼ˆä¸ä½¿ç”¨ FlashAttentionï¼Œä½¿ç”¨ PyTorch fallbackï¼‰
    output = attn(q, k, v)
    print(f"Prefill è¾“å…¥ Q: {q.shape}")
    print(f"Prefill è¾“å‡º: {output.shape}")
    assert output.shape == (num_tokens, num_heads, head_dim)
    
    reset_context()
    print("âœ… Attention ä¸ Context æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_store_kvcache():
    """æµ‹è¯• KV Cache å­˜å‚¨"""
    print("=" * 50)
    print("æµ‹è¯• store_kvcache")
    print("=" * 50)
    
    num_tokens = 6
    num_blocks = 4
    block_size = 4
    num_kv_heads = 2
    head_dim = 8
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    key = torch.randn(num_tokens, num_kv_heads, head_dim).cuda()
    value = torch.randn(num_tokens, num_kv_heads, head_dim).cuda()
    
    k_cache = torch.zeros(num_blocks, block_size, num_kv_heads, head_dim).cuda()
    v_cache = torch.zeros(num_blocks, block_size, num_kv_heads, head_dim).cuda()
    
    # slot mapping: å‡è®¾ tokens åˆ†å¸ƒåœ¨ block 1 (slots 4-7) å’Œ block 2 (slots 8-9)
    slot_mapping = torch.tensor([4, 5, 6, 7, 8, 9], device='cuda')
    
    # å­˜å‚¨
    store_kvcache(key, value, k_cache, v_cache, slot_mapping)
    
    # éªŒè¯
    k_cache_flat = k_cache.view(-1, num_kv_heads, head_dim)
    v_cache_flat = v_cache.view(-1, num_kv_heads, head_dim)
    
    for i, slot in enumerate(slot_mapping.tolist()):
        assert torch.allclose(k_cache_flat[slot], key[i]), f"Key mismatch at slot {slot}"
        assert torch.allclose(v_cache_flat[slot], value[i]), f"Value mismatch at slot {slot}"
    
    print(f"å­˜å‚¨äº† {num_tokens} ä¸ª token çš„ KV")
    print(f"K Cache éé›¶ slots: {(k_cache_flat.abs().sum(dim=(1,2)) > 0).sum().item()}")
    
    print("âœ… store_kvcache æµ‹è¯•é€šè¿‡!\n")


if __name__ == "__main__":
    test_block()
    test_block_manager_basic()
    test_block_manager_append()
    test_slot_mapping()
    test_prefix_cache()
    test_attention_with_context()
    
    # éœ€è¦ GPU çš„æµ‹è¯•
    if torch.cuda.is_available():
        test_store_kvcache()
    else:
        print("âš ï¸ è·³è¿‡ GPU æµ‹è¯• (store_kvcache)")
    
    print("=" * 50)
    print("ğŸ‰ Day 3 æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 50)